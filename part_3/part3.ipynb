{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "from part3 import *"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 76,
   "outputs": []
  },
  {
   "source": [
    "## Mathematical and Numerical Physics\n",
    "### Numerical part 3\n",
    "#### Kevin Vonk, s1706896, _Jan 2021_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## General part\n",
    "### Question 1\n",
    "#### a.\n",
    "\n",
    "The summation term in eq. (1) of the assignment has the same form as an element-wise matrix multiplication, but the indices are off. The general form of matrix multiplication $\\boldsymbol{C} = \\boldsymbol{A}\\boldsymbol{B}$ is (using the same indices as eq. (1) of the assignment),\n",
    "\\begin{align*}\n",
    "    c_{jk} = \\sum_{i=1}^n a_{ji}b_{ik}.\n",
    "\\end{align*}\n",
    "From this we see that the summation index is the column index of the first operand and the row index of the second operand. We can achieve this form in eq. (1) of the assignment by transposing the first operand,\n",
    "\\begin{align*}\n",
    "U_{ij} = U^T_{ji} = u'_{ji}.\n",
    "\\end{align*}\n",
    "So,\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^N u'_{ji}u_{ik} = c_{jk} = \\delta_{jk}.\n",
    "\\end{align*}\n",
    "From the delta we can deduce that when the indices $j$ and $k$ are equal, $c_{jk} = 1$. Otherwise, it is zero. These indices are only equal on the diagonal, so $\\boldsymbol{C} = \\boldsymbol{I}$. Changing from element-wise to general matrix products, we can write the final expression as,\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{U}^T\\boldsymbol{U} = \\boldsymbol{I},\n",
    "\\end{align*}\n",
    "which proves the equivalence.\n",
    "\n",
    "# TODO: Prove UU^T = I and statement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### b.\n",
    "The results and code generating the results are found below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\boldsymbol{U} = \\begin{bmatrix}0.01&-0.97&-0.25\\\\-0.75&0.16&-0.64\\\\0.66&0.19&-0.73\\end{bmatrix}$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Python reports:}\\quad\\boldsymbol{U}\\boldsymbol{U}^T = \\boldsymbol{U}^T\\boldsymbol{U} = \\boldsymbol{I}.$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "dim = 3\n",
    "\n",
    "U = ortho_group.rvs(dim)\n",
    "I = np.identity(dim)\n",
    "\n",
    "display(Math(r\"\\boldsymbol{U} = \" + to_latex(U, 2)))\n",
    "\n",
    "# Use np.allclose() to ignore IEEE 754 floating point rounding errors.\n",
    "if np.allclose(U @ U.T, I) and np.allclose(U.T @ U, I):\n",
    "    display(Math(r\"\\text{Python reports:}\\quad\\boldsymbol{U}\\boldsymbol{U}^T = \\boldsymbol{U}^T\\boldsymbol{U} = \\boldsymbol{I}.\"))\n",
    "else:\n",
    "    print(\"The statements i) and ii) do not hold.\")\n"
   ]
  },
  {
   "source": [
    "#### c.\n",
    "Firstly, lets prove symmetry. Starting of with the expression for $\\boldsymbol{B}$,\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{B} &= \\boldsymbol{U}^T \\boldsymbol{A} \\boldsymbol{U} \\\\\n",
    "    \\boldsymbol{B}^T &= (\\boldsymbol{U}^T \\boldsymbol{A} \\boldsymbol{U})^T = \\boldsymbol{U}^T \\boldsymbol{A}^T \\boldsymbol{U} = \\boldsymbol{U}^T \\boldsymbol{A} \\boldsymbol{U} \\\\\n",
    "    &\\rightarrow \\boldsymbol{B}^T = \\boldsymbol{B}.\n",
    "\\end{align*}\n",
    "\n",
    "So, $\\boldsymbol{B}$ is symmetric. Next, for the eigenvalues,\n",
    "\\begin{align*}\n",
    "    \\det(B) &= \\det(U^T A U) = \\det(U^T)\\det(A)\\det(U) \\\\\n",
    "    &= \\det(U^T)\\det(U)\\det(A) = \\det(U^T U)\\det(A) \\\\\n",
    "    &= \\det(I)\\det(A) \\\\\n",
    "    &= \\det(A).\n",
    "\\end{align*}\n",
    "And hence, by virtue of their determinants being identical, the eigenvalues $\\lambda$ of $\\boldsymbol{B}$ (determined by $\\det(B - \\lambda I) = 0$) will be identical too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### d."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Eigenvalues of}\\; \\boldsymbol{B} = \\begin{pmatrix}3.\\\\2.\\\\1.\\end{pmatrix}$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Eigenvectors of}\\; \\boldsymbol{B} = \\begin{pmatrix}0.66\\\\0.19\\\\-0.73\\end{pmatrix},\\begin{pmatrix}0.75\\\\-0.16\\\\0.64\\end{pmatrix},\\begin{pmatrix}-0.01\\\\0.97\\\\0.25\\end{pmatrix}$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "A = np.diag([1, 2, 3])\n",
    "B = U.T @ A @ U\n",
    "\n",
    "valB, vecB = np.linalg.eig(B)\n",
    "\n",
    "vecs = \",\".join([to_latex(vecB[:, i].reshape((3,1)), 2, \"pmatrix\") for i in range(len(valB))])\n",
    "\n",
    "display(Math(r\"\\text{Eigenvalues of}\\; \\boldsymbol{B} = \" + to_latex(valB.reshape((3,1)), 0, \"pmatrix\")))\n",
    "display(Math(r\"\\text{Eigenvectors of}\\; \\boldsymbol{B} = \" + vecs))"
   ]
  },
  {
   "source": [
    "What we can see from the results is that the eigenvalues of $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are identical. Additionally, the eigenvalues that have been found are the same values found on the diagonal of $\\boldsymbol{A}$ (which makes sense, as all the off-diagonal components are zero). Lastly, the eigenvectors of $\\boldsymbol{B}$ are rows of the $\\boldsymbol{U}$ matrix. In this case the eigenvector corresponding to the lowest eigenvalue is the first row in $\\boldsymbol{U}$, and the eigenvector corresponding to the highest eigenvalue is the last row, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 2\n",
    "#### a."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 3.0000; \\quad \\text{Eigenvector}=\\begin{bmatrix}0.66\\\\0.19\\\\-0.73\\end{bmatrix}; \\quad N_{iter} = 24$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "vec_start = rng.random(3)\n",
    "\n",
    "eig_val, eig_vec, iter = eigen_power(B, vec_start, ret_cycles=True)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1)) + r\"; \\quad N_{iter} = \" + f\"{iter}\"))"
   ]
  },
  {
   "source": [
    "We can see that the power method algorithm has found the largest eigenvalue and corresponding eigenvalue. The amount of iterations required to reach $\\epsilon < 10^{-8}$ is also given above. Note: I am deliberately not noting any numbers in this explaination text, since $\\boldsymbol{U}$ (and thus $\\boldsymbol{B}$ and its eigenvectors/eigenvalues) is based on random values which change in every run of the Notebook.\n",
    "\n",
    "The eigenvalue found using this method is not 100% accurate. Its accuracy is determined by floating point rounding errors and the given accuracy. We know that the analytical maximum eigenvalue of $\\boldsymbol{A}$ is $\\lambda_{ana,max} = 3$. So, we can compare the analytical value $\\lambda_{ana}$ to the value found using the Numpy library $\\lambda_{Python}$ and to the value found using the power method $\\lambda_{power}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle |\\lambda_{ana} - \\lambda_{Python}| = 1.332e-15$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle |\\lambda_{ana} - \\lambda_{power}| = 5.433e-09$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle |\\lambda_{Python} - \\lambda_{power}| = 5.433e-09$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "l_ana = 3\n",
    "diff_ana_python = np.abs(l_ana - np.max(valB))\n",
    "diff_ana_power = np.abs(l_ana - eig_val)\n",
    "diff_python_power = np.abs(np.max(valB) - eig_val)\n",
    "\n",
    "display(Math(r\"|\\lambda_{ana} - \\lambda_{Python}| = \" + f\"{diff_ana_python:.3e}\"))\n",
    "display(Math(r\"|\\lambda_{ana} - \\lambda_{power}| = \" + f\"{diff_ana_power:.3e}\"))\n",
    "display(Math(r\"|\\lambda_{Python} - \\lambda_{power}| = \" + f\"{diff_python_power:.3e}\"))"
   ]
  },
  {
   "source": [
    "From these results we find that there is no difference between the analytical eigenvalue and the eigenvalue determined by Numpy. However, there is a difference between these two and our implemented power method, in the order of the provided accuracy $10^{-8}$. So, this difference seems reasonable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### b.\n",
    "\n",
    "The same random starting vector as used in a) will be used again."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 1.0000; \\quad \\text{Eigenvector}=\\begin{bmatrix}-0.01\\\\0.97\\\\0.25\\end{bmatrix}; \\quad N_{iter} = 15$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "eig_val, eig_vec, iter = eigen_inv_power(B, vec_start, ret_cycles=True)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1)) + r\"; \\quad N_{iter} = \" + f\"{iter}\"))"
   ]
  },
  {
   "source": [
    "We can see from the results that we have computed the smallest eigenvalue and its corresponding eigenvector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### c.\n",
    "\n",
    "We will again use the same random starting vector as we have done at a) and b). According to the lecture notes sec. 2.3.1, \"If we choose $s$ close to $\\lambda$ ... the best guess for $\\lambda$ is then given by ...\",\n",
    "\n",
    "\\begin{align*}\n",
    "    \\lambda \\approx \\frac{1}{S_n} + s.\n",
    "\\end{align*}\n",
    "\n",
    "Since the last eigenvalue we are looking for is $\\lambda = 2$, we will need to use a shift that is somewhat close to it in order to find this eigenvalue using the shifted inverse power method. Lets assume we are able to make a somewhat reasonable guess of the eigenvalue, so we pick $s = 1.7$. The results are found below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 2.0000; \\quad \\text{Eigenvector}=\\begin{bmatrix}0.75\\\\-0.16\\\\0.64\\end{bmatrix}; \\quad N_{iter} = 11$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "eig_val, eig_vec, iter = eigen_shift_inv_power(B, vec_start, shift=1.7, ret_cycles=True)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1)) + r\"; \\quad N_{iter} = \" + f\"{iter}\"))"
   ]
  },
  {
   "source": [
    "We see that we have now obtained the last missing eigenvalue. Our value for the shift $s = 1.7$ seems to be a reasonable value since it brought us the correct eigenvalue we were looking for."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 3\n",
    "#### a."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}