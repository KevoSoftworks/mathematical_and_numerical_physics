{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, Math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "from part3 import *"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "## Mathematical and Numerical Physics\n",
    "### Numerical part 3\n",
    "#### Kevin Vonk, s1706896, _Jan 2021_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## General part\n",
    "### Question 1\n",
    "#### a.\n",
    "\n",
    "The summation term in eq. (1) of the assignment has the same form as an element-wise matrix multiplication, but the indices are off. The general form of matrix multiplication $\\boldsymbol{C} = \\boldsymbol{A}\\boldsymbol{B}$ is (using the same indices as eq. (1) of the assignment),\n",
    "\\begin{align*}\n",
    "    c_{jk} = \\sum_{i=1}^n a_{ji}b_{ik}.\n",
    "\\end{align*}\n",
    "From this we see that the summation index is the column index of the first operand and the row index of the second operand. We can achieve this form in eq. (1) of the assignment by transposing the first operand,\n",
    "\\begin{align*}\n",
    "U_{ij} = U^T_{ji} = u'_{ji}.\n",
    "\\end{align*}\n",
    "So,\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^N u'_{ji}u_{ik} = c_{jk} = \\delta_{jk}.\n",
    "\\end{align*}\n",
    "From the delta we can deduce that when the indices $j$ and $k$ are equal, $c_{jk} = 1$. Otherwise, it is zero. These indices are only equal on the diagonal, so $\\boldsymbol{C} = \\boldsymbol{I}$. Changing from element-wise to general matrix products, we can write the final expression as,\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{U}^T\\boldsymbol{U} = \\boldsymbol{I},\n",
    "\\end{align*}\n",
    "which proves the equivalence.\n",
    "\n",
    "# TODO: Prove UU^T = I and statement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### b.\n",
    "The results and code generating the results are found below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\boldsymbol{U} = \\begin{bmatrix}-0.48&0.14&-0.87\\\\0.76&0.55&-0.33\\\\0.43&-0.82&-0.37\\end{bmatrix}$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Python reports:}\\quad\\boldsymbol{U}\\boldsymbol{U}^T = \\boldsymbol{U}^T\\boldsymbol{U} = \\boldsymbol{I}.$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "dim = 3\n",
    "\n",
    "U = ortho_group.rvs(dim)\n",
    "I = np.identity(dim)\n",
    "\n",
    "display(Math(r\"\\boldsymbol{U} = \" + to_latex(U, 2)))\n",
    "\n",
    "# Use np.allclose() to ignore IEEE 754 floating point rounding errors.\n",
    "if np.allclose(U @ U.T, I) and np.allclose(U.T @ U, I):\n",
    "    display(Math(r\"\\text{Python reports:}\\quad\\boldsymbol{U}\\boldsymbol{U}^T = \\boldsymbol{U}^T\\boldsymbol{U} = \\boldsymbol{I}.\"))\n",
    "else:\n",
    "    print(\"The statements i) and ii) do not hold.\")\n"
   ]
  },
  {
   "source": [
    "#### c.\n",
    "Firstly, lets prove symmetry. Starting of with the expression for $\\boldsymbol{B}$,\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{B} &= \\boldsymbol{U}^T \\boldsymbol{A} \\boldsymbol{U} \\\\\n",
    "    \\boldsymbol{B}^T &= (\\boldsymbol{U}^T \\boldsymbol{A} \\boldsymbol{U})^T = \\boldsymbol{U}^T \\boldsymbol{A}^T \\boldsymbol{U} = \\boldsymbol{U}^T \\boldsymbol{A} \\boldsymbol{U} \\\\\n",
    "    &\\rightarrow \\boldsymbol{B}^T = \\boldsymbol{B}.\n",
    "\\end{align*}\n",
    "\n",
    "So, $\\boldsymbol{B}$ is symmetric. Next, for the eigenvalues,\n",
    "\\begin{align*}\n",
    "    \\det(B) &= \\det(U^T A U) = \\det(U^T)\\det(A)\\det(U) \\\\\n",
    "    &= \\det(U^T)\\det(U)\\det(A) = \\det(U^T U)\\det(A) \\\\\n",
    "    &= \\det(I)\\det(A) \\\\\n",
    "    &= \\det(A).\n",
    "\\end{align*}\n",
    "And hence, by virtue of their determinants being identical, the eigenvalues $\\lambda$ of $\\boldsymbol{B}$ (determined by $\\det(B - \\lambda I) = 0$) will be identical too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### d."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Eigenvalues of}\\; \\boldsymbol{B} = \\begin{pmatrix}1.\\\\2.\\\\3.\\end{pmatrix}$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Eigenvectors of}\\; \\boldsymbol{B} = \\begin{pmatrix}0.48\\\\-0.14\\\\0.87\\end{pmatrix},\\begin{pmatrix}0.76\\\\0.55\\\\-0.33\\end{pmatrix},\\begin{pmatrix}0.43\\\\-0.82\\\\-0.37\\end{pmatrix}$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "A = np.diag([1, 2, 3])\n",
    "B = U.T @ A @ U\n",
    "\n",
    "valB, vecB = np.linalg.eig(B)\n",
    "\n",
    "vecs = \",\".join([to_latex(vecB[:, i].reshape((3,1)), 2, \"pmatrix\") for i in range(len(valB))])\n",
    "\n",
    "display(Math(r\"\\text{Eigenvalues of}\\; \\boldsymbol{B} = \" + to_latex(valB.reshape((3,1)), 0, \"pmatrix\")))\n",
    "display(Math(r\"\\text{Eigenvectors of}\\; \\boldsymbol{B} = \" + vecs))"
   ]
  },
  {
   "source": [
    "What we can see from the results is that the eigenvalues of $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are identical. Additionally, the eigenvalues that have been found are the same values found on the diagonal of $\\boldsymbol{A}$ (which makes sense, as all the off-diagonal components are zero). Lastly, the eigenvectors of $\\boldsymbol{B}$ are rows of the $\\boldsymbol{U}$ matrix. In this case the eigenvector corresponding to the lowest eigenvalue is the first row in $\\boldsymbol{U}$, and the eigenvector corresponding to the highest eigenvalue is the last row, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 2\n",
    "#### a."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 3.0000; \\quad \\text{Eigenvector}=\\begin{pmatrix}-0.43\\\\0.82\\\\0.37\\end{pmatrix}; \\quad N_{iter} = 25$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "vec_start = rng.random(3)\n",
    "\n",
    "eig_val, eig_vec, iter = eigen_power(B, vec_start, ret_cycles=True)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1), type=\"pmatrix\") + r\"; \\quad N_{iter} = \" + f\"{iter}\"))"
   ]
  },
  {
   "source": [
    "We can see that the power method algorithm has found the largest eigenvalue and corresponding eigenvalue. The amount of iterations required to reach $\\epsilon < 10^{-8}$ is also given above. Note: I am deliberately not noting any numbers in this explaination text, since $\\boldsymbol{U}$ (and thus $\\boldsymbol{B}$ and its eigenvectors/eigenvalues) is based on random values which change in every run of the Notebook.\n",
    "\n",
    "The eigenvalue found using this method is not 100% accurate. Its accuracy is determined by floating point rounding errors and the given accuracy. We know that the analytical maximum eigenvalue of $\\boldsymbol{A}$ is $\\lambda_{ana,max} = 3$. So, we can compare the analytical value $\\lambda_{ana}$ to the value found using the Numpy library $\\lambda_{Python}$ and to the value found using the power method $\\lambda_{power}$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle |\\lambda_{ana} - \\lambda_{Python}| = 1.332e-15$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle |\\lambda_{ana} - \\lambda_{power}| = 6.669e-09$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle |\\lambda_{Python} - \\lambda_{power}| = 6.669e-09$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "l_ana = 3\n",
    "diff_ana_python = np.abs(l_ana - np.max(valB))\n",
    "diff_ana_power = np.abs(l_ana - eig_val)\n",
    "diff_python_power = np.abs(np.max(valB) - eig_val)\n",
    "\n",
    "display(Math(r\"|\\lambda_{ana} - \\lambda_{Python}| = \" + f\"{diff_ana_python:.3e}\"))\n",
    "display(Math(r\"|\\lambda_{ana} - \\lambda_{power}| = \" + f\"{diff_ana_power:.3e}\"))\n",
    "display(Math(r\"|\\lambda_{Python} - \\lambda_{power}| = \" + f\"{diff_python_power:.3e}\"))"
   ]
  },
  {
   "source": [
    "From these results we find that there is no difference between the analytical eigenvalue and the eigenvalue determined by Numpy. However, there is a difference between these two and our implemented power method, in the order of the provided accuracy $10^{-8}$. So, this difference seems reasonable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### b.\n",
    "\n",
    "The same random starting vector as used in a) will be used again."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 1.0000; \\quad \\text{Eigenvector}=\\begin{pmatrix}0.48\\\\-0.14\\\\0.87\\end{pmatrix}; \\quad N_{iter} = 12$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "eig_val, eig_vec, iter = eigen_inv_power(B, vec_start, ret_cycles=True)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1), type=\"pmatrix\") + r\"; \\quad N_{iter} = \" + f\"{iter}\"))"
   ]
  },
  {
   "source": [
    "We can see from the results that we have computed the smallest eigenvalue and its corresponding eigenvector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### c.\n",
    "\n",
    "We will again use the same random starting vector as we have done at a) and b). According to the lecture notes sec. 2.3.1, \"If we choose $s$ close to $\\lambda$ ... the best guess for $\\lambda$ is then given by ...\",\n",
    "\n",
    "\\begin{align*}\n",
    "    \\lambda \\approx \\frac{1}{S_n} + s.\n",
    "\\end{align*}\n",
    "\n",
    "Since the last eigenvalue we are looking for is $\\lambda = 2$, we will need to use a shift that is somewhat close to it in order to find this eigenvalue using the shifted inverse power method. Lets assume we are able to make a somewhat reasonable guess of the eigenvalue, so we pick $s = 1.7$. The results are found below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 2.0000; \\quad \\text{Eigenvector}=\\begin{pmatrix}0.76\\\\0.55\\\\-0.33\\end{pmatrix}; \\quad N_{iter} = 14$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "eig_val, eig_vec, iter = eigen_shift_inv_power(B, vec_start, shift=1.7, ret_cycles=True)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1), type=\"pmatrix\") + r\"; \\quad N_{iter} = \" + f\"{iter}\"))"
   ]
  },
  {
   "source": [
    "We see that we have now obtained the last missing eigenvalue. Our value for the shift $s = 1.7$ seems to be a reasonable value since it brought us the correct eigenvalue we were looking for."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 3\n",
    "#### a."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\text{Numpy eigenvectors}=\\begin{pmatrix}0.&\\\\-0.99\\\\-0.16\\end{pmatrix},\\begin{pmatrix}0.8&\\\\-0.33\\\\-0.49\\end{pmatrix}; \\quad \\text{Power iteration eigenvector}=\\begin{pmatrix}0.5&\\\\0.86\\\\-0.14\\end{pmatrix}; \\quad \\lambda_{power}=2.0000$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "C = np.diag(np.array([1, 2, 2]))\n",
    "B = U.T @ C @ U\n",
    "\n",
    "eig_val, eig_vec = eigen_power(B, vec_start)\n",
    "np_eig_val, np_eig_vec = np.linalg.eig(B)\n",
    "vecs = \",\".join([to_latex(np_eig_vec[:, i].reshape((3,1)), 2, \"pmatrix\") for i in range(len(valB)) if np.isclose(np_eig_val[i], 2)])\n",
    "\n",
    "display(Math(r\"\\text{Numpy eigenvectors}=\" + vecs + r\"; \\quad \\text{Power iteration eigenvector}=\" + to_latex(eig_vec.reshape(3,1), type=\"pmatrix\") + r\"; \\quad \\lambda_{power}=\" f\"{eig_val:.4f}\"))"
   ]
  },
  {
   "source": [
    "We can see that while the found eigenvalue is what we would theoretically expect ($\\lambda_{max} = 2$), the eigenvector result is peculiar as it indeed does not correspond to vectors found using Numpy. The reason that the found eigenvector is invalid is because we have degeneracy in the eigenvalues, but nowhere in the algorithm do we actually account for this. To be more precise, the whole power iteration algorithm relies on the fact that eigenvalues are ordered from largest to smallest, $|\\lambda_n| > |\\lambda_{n+1}|$ for $n = 1, ..., N-1$ (eq. (2.5) in the lecture notes). Note specifically here that we use greater than, meaning that the two eigenvalues cannot be equal. Because the requirements for the power iteration are no longer satisfied, the results found using it are completely baseless (and thus wrong). This also implies that the found eigenvalue is wrong, even though it does provide the correct answer in this case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### b."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{numpy} = \\begin{pmatrix}-2.&2.&1.\\end{pmatrix}; \\quad \\text{Numpy eigenvectors}=\\begin{pmatrix}0.43\\\\-0.82\\\\-0.37\\end{pmatrix},\\begin{pmatrix}-0.76\\\\-0.55\\\\0.33\\end{pmatrix},\\begin{pmatrix}0.48\\\\-0.14\\\\0.87\\end{pmatrix}$"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{power}=1.2385; \\quad \\text{Power iteration eigenvector}=\\begin{pmatrix}0.5&\\\\0.86\\\\-0.14\\end{pmatrix}$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "C = np.diag(np.array([1, 2, -2]))\n",
    "B = U.T @ C @ U\n",
    "\n",
    "eig_val, eig_vec = eigen_power(B, vec_start)\n",
    "np_eig_val, np_eig_vec = np.linalg.eig(B)\n",
    "vecs = \",\".join([to_latex(np_eig_vec[:, i].reshape((3,1)), 2, \"pmatrix\") for i in range(len(valB))])\n",
    "\n",
    "display(Math(r\"\\lambda_{numpy} = \" + to_latex(np_eig_val, type=\"pmatrix\") + r\"; \\quad \\text{Numpy eigenvectors}=\" + vecs))\n",
    "display(Math(r\"\\lambda_{power}=\" f\"{eig_val:.4f}\" + r\"; \\quad \\text{Power iteration eigenvector}=\" + to_latex(eig_vec.reshape(3,1), type=\"pmatrix\")))"
   ]
  },
  {
   "source": [
    "We find here that Numpy is able to compute the correct eigenvalues and corresponding eigenvectors. The power method fails completely here, yielding both an incorrect eigenvalue as eigenvector. This is what we predicted in a) already, where \"by chance\" the found eigenvalue was still correct. The reason the power method fails again has to do with the constraint $|\\lambda_n| > |\\lambda_{n+1}|$ for $n = 1, ..., N-1$. Note that we take the absolute values of the eigenvalues, which causes the problem this time. As $|-2| = |2|$, the constraint is not upheld and we cannot use the power method.\n",
    "\n",
    "So, the power method fails when any of the (absolute) eigenvalues is degenerate. One of the ways we could check if the result is reasonable is by using our conclusions determined at 1d). There, we determined that the eigenvalues are found in the diagonal $\\boldsymbol{A}$ matrix (or $\\boldsymbol{C}$ or $\\boldsymbol{F}$ in these cases). Additionally, the found eigenvectors correspond to rows of the $\\boldsymbol{U}$ matrix.\n",
    "\n",
    "Another way to test whether the answer is reasonable, or if we don't know any of the previous matrices, is by testing various relations that must hold for eigenvalues and eigenvectors. We know for example that $\\boldsymbol{A}\\vec{v} = \\lambda\\vec{v}$ for a matrix $\\boldsymbol{A}$ with eigenvector $\\vec{v}$ and eigenvalue $\\lambda$. Let us check whether or not this relation holds for the power iteration result:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The eigenvalue/eigenvector relation does not hold for the found results\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(C @ eig_vec, eig_val * eig_vec):\n",
    "    print(\"The eigenvalue/eigenvector relation holds\")\n",
    "else:\n",
    "    print(\"The eigenvalue/eigenvector relation does not hold for the found results\")"
   ]
  },
  {
   "source": [
    "Bingo! We can indeed verify that our found results are wrong. A way we could work around this problem is by introducting an offset $\\delta$. If we add or subtract this offset from the values which cause the degeneracy, we are able to lift it. This does introduce a new error in both the found eigenvalue and eigenvector, but we could use these values as guesses in an algorithm which circumvents the problem entirely, but takes a while to converge. As an example,"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Math object>",
      "text/latex": "$\\displaystyle \\lambda_{max} = 2.0500; \\quad \\text{Eigenvector}=\\begin{pmatrix}0.76\\\\0.55\\\\-0.33\\end{pmatrix}$"
     },
     "metadata": {}
    }
   ],
   "source": [
    "delta = 0.05\n",
    "\n",
    "C = np.diag(np.array([1, 2 + delta, -2 + delta]))\n",
    "B = U.T @ C @ U\n",
    "\n",
    "eig_val, eig_vec = eigen_power(B, vec_start)\n",
    "\n",
    "display(Math(r\"\\lambda_{max} = \" + f\"{eig_val:.4f}\" + r\"; \\quad \\text{Eigenvector}=\" + to_latex(eig_vec.reshape(3,1), type=\"pmatrix\")))"
   ]
  },
  {
   "source": [
    "if np.allclose(np.around(B @ eig_vec, 2), np.around(eig_val * eig_vec, 2)):\n",
    "    print(\"The eigenvalue/eigenvector relation holds\")\n",
    "else:\n",
    "    print(\"The eigenvalue/eigenvector relation does not hold for the found results\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The eigenvalue/eigenvector relation holds\n"
     ]
    }
   ]
  },
  {
   "source": [
    "The accuracy of this is poor however, in this case I have rounded the results to two decimals in order for the relation to hold.\n",
    "\n",
    "A much better way to avoid this problem would be to construct a new basis in which the degeneracy is lifted, much like what is done for degenerate perturbation theory in Quantum Mechanics. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}