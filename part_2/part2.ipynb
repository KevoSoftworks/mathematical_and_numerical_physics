{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from part2_lu import *"
   ]
  },
  {
   "source": [
    "## Mathematical and Numerical Physics\n",
    "### Numerical part 2\n",
    "#### Kevin Vonk, s1706896, _Dec 2020 - Jan 2021_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## General Part\n",
    "### Question 1\n",
    "#### a.\n",
    "Firstly, let us write out the component-wise product $\\boldsymbol{T} = \\boldsymbol{L'}\\boldsymbol{U'}$, where $i = 2, ..., N$,\n",
    "\\begin{align*}\n",
    "    t_{1,1} &= l'_{1,1} \\\\\n",
    "    t_{i, i-1} &= l'_{i, i-1} \\\\\n",
    "    t_{i, i} &= l'_{i, i-1} \\cdot u'_{i-1, i} + l'_{i, i} \\\\\n",
    "    t_{i-1, i} &= l'_{i-1, i-1} \\cdot u'_{i-1, i}.\n",
    "\\end{align*}\n",
    "\n",
    "Now that we have these expressions, we can define the form of $\\boldsymbol{L'}$ and $\\boldsymbol{U'}$ analogously to eq. (1.7) of the lecture notes,\n",
    "\\begin{equation*}\n",
    "    l'_{1, 1} = t_{1, 1}; \\; \\text{for} \\; i = 2, ..., N, \\; u'_{i-1, i} = \\frac{t_{i-1, 1}}{l'_{i-1, i-1}}; \\; l'_{i, i-1} = t_{i, i-1}; \\; l'_{i, i} = t_{i, i} - l'_{i, i-1} \\cdot u'_{i-1, i}.\n",
    "\\end{equation*}\n",
    "\n",
    "We still perform the forward subsitution as $\\boldsymbol{L'}\\boldsymbol{y} = \\boldsymbol{b}$ as in eq. (1.8) in the lecture notes,\n",
    "\\begin{equation*}\n",
    "    y_1 = \\frac{b_1}{l'_{1, 1}}; \\; \\text{for} \\; i = 2, ..., N \\; y_i = \\frac{b_i}{l'_{i, i}} - l'_{i, i-1}y_{i-1}.\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, for the back subsitution step $\\boldsymbol{U}\\boldsymbol{x} = \\boldsymbol{y}$ we again follow the same procedure as in eq. (1.9) of the lecture notes,\n",
    "\\begin{equation*}\n",
    "    x_N = y_N; \\; \\text{for} \\; i = N, ..., 2 \\; x_{i-1} = y_{i-1} - u'_{i-1, i}x_i.\n",
    "\\end{equation*}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### b.\n",
    "_Case I_\n",
    "\n",
    "We can naively solve this system by writing out the two linear equations,\n",
    "\\begin{align*}\n",
    "    x_1 + x_2 &= 1 \\\\\n",
    "    x_1 + (1 + 10^{-10})x_2 &= 1.\n",
    "\\end{align*}\n",
    "\n",
    "Filling in one into the other yields,\n",
    "\\begin{align*}\n",
    "    1 - x_2 + (1 + 10^{-10})x_2 &= 1 \\\\\n",
    "    10^{-10}x_2 &= 0 \\\\\n",
    "    x_2 &= 0 \\\\\n",
    "    &\\rightarrow x_1 = 1.\n",
    "\\end{align*}\n",
    "\n",
    "So,\n",
    "\\begin{equation*}\n",
    "    \\vec{x} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "\n",
    "_Case II_\n",
    "\n",
    "Similarly as above,\n",
    "\\begin{align*}\n",
    "    x_1 + x_2 &= 1 \\\\\n",
    "    x_1 + (1 + 10^{-10})x_2 &= 1 + 10^{-10}.\n",
    "\\end{align*}\n",
    "\n",
    "Again, filling in one into the other,\n",
    "\\begin{align*}\n",
    "    1 - x_2 + (1 + 10^{-10})x_2 &= 1 \\\\\n",
    "    10^{-10}x_2 &= 10^{-10} \\\\\n",
    "    x_2 &= 1 \\\\\n",
    "    &\\rightarrow x_1 = 0.\n",
    "\\end{align*}\n",
    "\n",
    "So,\n",
    "\\begin{equation*}\n",
    "    \\vec{x} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}.\n",
    "\\end{equation*}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### c.\n",
    "Before even having calculated anything I am certain this matrix is ill-conditioned, purely due to the $10^{-10}$ term present. Let us prove this however by computing the eigenvalues of this matrix,\n",
    "\\begin{align*}\n",
    "    \\begin{vmatrix}\n",
    "        1 - \\lambda & 1 \\\\\n",
    "        1 & 1 + 10^{-10} - \\lambda\n",
    "    \\end{vmatrix} &= 0 \\\\\n",
    "    (1 - \\lambda)(1 + 10^{-10} - \\lambda) - 1 &= 0 \\\\\n",
    "    \\lambda^2 -\\lambda(2 + 10^{-10}) + 10^{-10} &= 0 \\\\\n",
    "    \\lambda_\\pm = \\frac{2 + 10^{-10} \\pm \\sqrt{(2 + 10^{-10})^2 - 4*10^{-10}}}{2} &\\\\\n",
    "    \\lambda_\\pm = \\frac{2 + 10^{-10} \\pm \\sqrt{4 + 10^{-20}}}{2}. &\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, these two eigenvalues turn out to be vastly different from one another, about an expected order of magnitude $10^{-10}$. Now, it doesn't yield anything by writing out these expressions further, nor is it easy and useful to simplify this further. We can however, give the approximate values:\n",
    "\\begin{align*}\n",
    "    \\lambda_{\\text{max}} \\approx 2; \\; \\lambda_{\\text{min}} \\approx 5\\cdot 10^{-11}.\n",
    "\\end{align*}\n",
    "\n",
    "Filling in these values into the expression for the condition number yields,\n",
    "\\begin{align*}\n",
    "    \\kappa \\approx \\frac{2}{5\\cdot 10^{-11}} \\approx 4\\cdot 10^{10}.\n",
    "\\end{align*}\n",
    "\n",
    "Given that $\\kappa$ should idealy lie around $1$, to say that this matrix is ill-conditioned is an understatement. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### d.\n",
    "What we have here is a nearly singular matrix ($\\begin{bmatrix}1 & 1 \\\\ 1 & 1\\end{bmatrix}$ is a fully singular matrix, and we are only $10^{-10}$ away from this in only a single element). We have already established that this problem is ill-conditioned, and indeed, when changing the independent variables only slightly (case I to case II), the dependent variable $\\vec{x}$ changes dramatically. From the definition of the condition number (eq. (1.13) of the lecture notes), we learn that it is related to the maximum relative error. Given that the condition number is in the order of $10^{10}$ for this problem, and assuming that for the uncertainty $\\delta$ of $b$ it holds that $\\delta < b$ (which is a more than reasonable assumption), the error is so incredibly large that the solution for $\\vec{x}$ is useless.\n",
    "\n",
    "Now, if $\\vec{b}$ were some inputs to a model and $\\vec{x}$ the output parameters, we can deduce from what has previously been stated that a tiny change in the input (possibly even rounding errors) will lead to a completely different output of the model. This leads us to conclude that convergence is (nearly) impossible (i.e. we will not find a solution to $\\vec{x}$ to a reasonable degree, if at all), or that the problem is ill-conditioned (i.e. the parameters that we need to work with cannot produce a satisfying result)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Question 2\n",
    "#### a.\n",
    "The implementation that I have written stores the tridiagonal LU decomposition in $3N - 1$ units of memory. This is a little worse than the theoretical $3N - 2$ if we assume that only array elements take memory. The reason for this is that instead of having three arrays, one for the off-diagonal $L$, one for the diagonal $U$ and one for the off-diagonal $U$, I have a 1-D array of length $N - 1$ for $L$, and a 2-D array of length $2 \\times N$ for $U$. Since the off-diagonal of $U$ only has $N - 1$ elements, the last element of this 2-D array is always zero, and thus wasted. However, in an actual memory map this method will be more efficient than the three separate arrays, since there is less header data and/or metadata needed to define the arrays.\n",
    "\n",
    "Now, let us consider the given matrix $T$. From the tridiagonal decomposition (eqs. (1.7) - (1.9) in the lecture notes) we can state the following,\n",
    "\\begin{align*}\n",
    "    u_{i-1, i} &= t_{i-1, i} = 1 \\\\\n",
    "    u_{i, i} &= t_{i, i} - l_{i, i-1}u_{i-1,i} \\\\\n",
    "    &= -2 - u_{i-1, i-1}^{-1} &\\quad \\text{(filling in the general expression for $l_{i, i-1}$)}\\\\\n",
    "    l_{i, i-1} &= t_{i, i-1} \\cdot u_{i-1, i-1}^{-1} \\\\\n",
    "    &= -(2 + u_{i-2, i-2}^{-1})^{-1} &\\quad \\text{(using the found relation for the diagonal of $U$)}\\\\\n",
    "    &= -(2 + l_{i-1, i-2})^{-1} &\\quad \\text{(using the general expression stated above, $l_{i, i-1} = ...$)}\n",
    "\\end{align*}\n",
    "\n",
    "Lastly, let us check if these relations are identical to the result we obtain from our written tridiagonal LU decomposition. The Python code used to verify this is found below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The recurrence relations hold.\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "diag = -2 * np.ones(N)\n",
    "off_diag = np.ones(N-1)\n",
    "\n",
    "# Decompose the matrix\n",
    "l, u = SparseTridiagonal().decompose(diag, off_diag)\n",
    "\n",
    "# Compare to the recurrence relations\n",
    "res_l = l == -1 / (2 + np.roll(l, 1))\n",
    "res_u = u[0, :] == -2 - 1/np.roll(u[0], 1)\n",
    "\n",
    "# Check if the recurrence relations hold for i = range(1, N)\n",
    "if res_l[1:].all() and res_u[1:].all():\n",
    "    print(\"The recurrence relations hold.\")\n",
    "else:\n",
    "    print(\"The recurrence relations do not agree with the tridiagonal decomposition.\")"
   ]
  },
  {
   "source": [
    "As we can see from the script's result, the recurrence relations hold for $i \\in {2, ..., N}$ (in 1-based indexing). The reason that we skip the first index is because for $i = 1$, $l_{i, i-1}$ is not defined (or out of bounds)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}